---
layout: post
title: You're up and running!
---

# Bayesian Nonparametric Kernel Learning 

## Problem 

* Fixed Kernel is ad-hoc, and not data-driven. 
* Due to $$$N\times N$$$ Gram matrix computation, cannot be applicable to large datasets. 

## Prelim 

### Random features (RKS)

__Bochner Theorem.__ _A continuous kernel $k(\mathbf{x},\mathbf{y}) = k(\mathbf{x}-\mathbf{y})$ on $\mathbb{R}^d$ is positive definite if and only if $k(\delta)$ is the Fourier trnasform of a non-negative measure_.

That is, Bochner's theorem gurantees 

$$ 
\begin{split}
k(\mathbf{x},\mathbf{y}) &= k(\mathbf{x}-\mathbf{y}) \\ 
&= \int_{\mathbb{R}^d} p(\omega) e^{i w'(\mathbf{x}-\mathbf{y})}d\omega \\ 
&= E_\omega \left[ \tau_\omega(\mathbf{x}), \tau_\omega(\mathbf{y}) \right]
\end{split}
$$
where $\tau_\omega(\mathbf{x}) = e^{iw'\mathbf{x}} $. If $k(\cdot)$ and $p(\cdot)$ is real value, the integral converges to real, and the complex exponentials are replicaed with cosines. Thus, we can rewrite as follow: 

$$ 
\begin{split}
k(\mathbf{x},\mathbf{y}) &= E\left[ z_\omega(\mathbf{x})z_\omega(\mathbf{y}) \right]
\end{split}
$$
where $z_\omega(\mathbf{x}) = \sqrt{2}\cos(\omega'\mathbf{x}+b) $, and $\omega \sim p(\omega)$, and $b \sim U(0,2\pi)$. If we can properly draw the samples like this, the above integral can be computed using MC simulation. 

## Model motivation (Specific idea)

The point in RKS model is that given a shift invariant kernel $K(\mathbf{x},\mathbf{y}) = k(\mathbf{x}-\mathbf{y})$, one can constructs an approximate primal space to estimate kernel evaluations as dot product of $\tau_\omega(\mathbf{x})$, where $\tau(\cdot)$ can be constructed by random Fourier trnasformation, drawn from $p(\omega)$, that is defined by the given kernel function. That is, the probablistic function $p(\omega)$ is defined the given kernel function; in serial, the random frequency $\tau_\omega(\cdot)$ is drawn from the $p(\omega)$. 

In other words, where the random frequency drawn also is of the $p(\omega)$ originated. Those random distribution actually defines a kernel. To learn kernels by varying the given data, the distribution $p(\omega)$ should be flexible. 

To do so, the random distribution $p(\omega)$ is varied. A prior of $p(\omega)$ is chosen as DPMM. 

## Model 

  


## Key papers
 
* A. Rahimi and B. Recht. Random features for large-scale kernel machines, NIPS, 2007
* A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning, NIPS, 2007

## Taste?

### Extended? 

### Applicable? 
